#!/usr/bin/env python
# coding: utf-8

# In[43]:


####### Projet IA - Analyse D√©mographique et Pr√©diction de l‚ÄôEsp√©rance de Vie ######

# √âcole : HESTIM  &  INSA Hauts-de-France
# Nom et Pr√©nom : YLEWO Arrolle I√ßaire  
# Unit√© d'enseignement :  Intelligence Artificielle  
# Ann√©e acad√©mique : 2024-2025  
# Classe : Master 2 Cyberd√©fense & S√©curit√© de l'Information (CDSI)


# In[44]:


# 1. Introduction & Objectifs

## Pr√©sentation du sujet;
# L'esp√©rance de vie est un indicateur cl√© du d√©veloppement humain, influenc√© par de nombreux facteurs socio-√©conomiques, 
# sanitaires et environnementaux. √Ä travers ce projet, nous analysons un ensemble de donn√©es d√©mographiques provenant de 
# diff√©rents pays afin de mieux comprendre les variables qui influencent l'esp√©rance de vie.

## Objectifs du projet:
# - Exploration des donn√©es : R√©aliser une analyse exploratoire pour identifier les tendances et corr√©lations importantes.
# - Clustering : Regrouper les pays selon leurs caract√©ristiques d√©mographiques et sanitaires afin d'identifier des profils types.
# - R√©gression : Construire un mod√®le pr√©dictif capable d'estimer l'esp√©rance de vie d'un pays √† partir de ses indicateurs √©conomiques et sanitaires.

## Justification de l'int√©r√™t du sujet:
# La compr√©hension des facteurs d√©terminants de l'esp√©rance de vie est cruciale pour orienter les politiques publiques 
# en mati√®re de sant√©, d'√©ducation et de d√©veloppement. Ce projet permet non seulement de mettre en pratique des techniques 
# d'analyse de donn√©es et d'intelligence artificielle, mais aussi de contribuer √† une meilleure compr√©hension des enjeux 
# globaux li√©s √† la qualit√© de vie des populations.


# In[45]:


# 2. Importation des Biblioth√®ques


# In[46]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster


# In[47]:


# 3. & Chargement des Donn√©es & Description du Dataset

# source : https://www.kaggle.com/datasets/nelgiriyewithana/countries-of-the-world-2023/data
# Ce dataset mondial contient des informations vari√©es sur chaque pays : 
# indicateurs d√©mographiques, √©conomiques, sanitaires, √©ducatifs, etc.


# In[48]:


#importation du dataset
df = pd.read_csv ("world-data-2023.csv")
print("######  chargement corect du Dataset")

# Aper√ßu des premi√®res lignes du dataset
print("######  Aper√ßu du dataset :")
display(df.head())

# Dimensions du dataset
print(f"######  Dimensions : {df.shape[0]} lignes et {df.shape[1]} colonnes.\n")

# Types de variables
print("######  Types de donn√©es :")
print(df.dtypes)

# Valeurs manquantes (en % par colonne)
print("\n‚ùó Pourcentage de valeurs manquantes :")
missing_values = df.isnull().mean() * 100
print(missing_values[missing_values > 0].sort_values(ascending=False))

# Statistiques descriptives
print("\n######  Statistiques descriptives :")
display(df.describe())

# Visualisation : Histogrammes des principales variables
variables_a_plot = ['Life expectancy', 'GDP', 'Fertility Rate', 'Infant mortality', 'Physicians per thousand']
df[variables_a_plot].hist(bins=30, figsize=(15,10), color='skyblue', edgecolor='black')
plt.suptitle("Distribution des variables principales")
plt.tight_layout()
plt.show()

# Visualisation : Boxplots pour d√©tecter les outliers
plt.figure(figsize=(15,8))
for i, var in enumerate(variables_a_plot, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(x=df[var], color='lightcoral')
    plt.title(f'Boxplot de {var}')
plt.tight_layout()
plt.show()


# In[49]:


# 4. Pr√©traitement des Donn√©es
#  Suppression ou imputation des valeurs manquantes
#  Encodage des variables cat√©gorielles (si besoin)
#  Normalisation / Standardisation
#  Sauvegarde du dataset nettoy√© (optionnel)


# In[50]:


## Nettoyage des valeurs 

def clean_numeric_column(col):
    col = col.astype(str)

    # Corrige les notations scientifiques malform√©es comme "1.23-04" => "1.23e-04"
    col = col.str.replace(r'(?<=\d)-(?=\d{2,}$)', r'e-', regex=True)

    # Supprime tous les caract√®res sauf chiffres, points, "e", "-" (pour les exposants n√©gatifs)
    col = col.str.replace(r'[^0-9eE\.-]', '', regex=True)

    # Remplace les cha√Ænes vides par NaN
    col = col.replace('', np.nan)

    return pd.to_numeric(col, errors='coerce')


# Colonnes √† nettoyer (extraites de ton exemple, √† ajuster selon le dataset complet)
cols_to_clean = [
    'Agricultural Land( %)', 'Land Area(Km2)', 'Armed Forces size', 'Birth Rate', 'Co2-Emissions',
    'CPI', 'CPI Change (%)', 'Fertility Rate', 'Forested Area (%)', 'Gasoline Price', 'GDP',
    'Gross primary education enrollment (%)', 'Gross tertiary education enrollment (%)', 
    'Infant mortality', 'Life expectancy', 'Maternal mortality ratio', 'Minimum wage', 
    'Out of pocket health expenditure', 'Physicians per thousand', 'Population',
    'Population: Labor force participation (%)', 'Tax revenue (%)', 'Total tax rate',
    'Unemployment rate', 'Urban_population', 'Latitude', 'Longitude']

for col in cols_to_clean:
    if col in df.columns:
        df[col] = clean_numeric_column(df[col])


## Traitement des valeurs manquantes

# Imputation moyenne (pour les colonnes num√©riques)
num_cols = df.select_dtypes(include=np.number).columns.tolist()
imputer = SimpleImputer(strategy='mean')
df[num_cols] = imputer.fit_transform(df[num_cols])


## Normalisation (StandardScaler)
scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

# Maintenant, df est pr√™t pour l‚Äôanalyse ou le machine learning
print(df.head())


# Affichage des colonnes avec valeurs manquantes
missing_values = df.isnull().sum()
print("Valeurs manquantes :\n", missing_values[missing_values > 0])


# In[51]:


# 5. Analyse Exploratoire des Donn√©es (EDA)
# Heatmap de corr√©lation
# Scatterplots : Life Expectancy vs autres variables
# GroupBy et boxplots par cat√©gories (continent, revenus, etc.)
# Analyse multivari√©e pour identifier les facteurs influents


# Heatmap : Visualise les corr√©lations entre les variables num√©riques.
# Scatterplots : Montre la relation entre l'esp√©rance de vie et les variables les plus corr√©l√©es.
# Boxplots : Compare l'esp√©rance de vie par cat√©gories (continent, groupe de revenus, etc.).
# Pairplot : Explore les relations multivari√©es entre l'esp√©rance de vie et les variables influentes.
# R√©gression lin√©aire multiple : Pr√©dit l'esp√©rance de vie en fonction des variables les plus corr√©l√©es.


# In[52]:


# Heatmap de corr√©lation

# Calcul de la matrice de corr√©lation
corr_matrix = df.corr(numeric_only=True)

# Heatmap
plt.figure(figsize=(15, 12))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True, 
            cbar_kws={"shrink": .8}, linewidths=0.5)
plt.title('Heatmap de corr√©lation')
plt.tight_layout()
plt.show()


# Scatterplots : Life Expectancy vs autres variables cl√©s
# Top 6 variables les plus corr√©l√©es √† Life expectancy (positives et n√©gatives)
target = 'Life expectancy'
correlations = corr_matrix[target].drop(target).sort_values(key=abs, ascending=False).head(6)

# Scatterplots
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()
for i, var in enumerate(correlations.index):
    sns.scatterplot(data=df, x=var, y=target, ax=axes[i])
    axes[i].set_title(f'{target} vs {var}')
plt.tight_layout()
plt.show()

# Scatter plot pour la densit√© de population (Density(P/Km2))
if 'Density\n(P/Km2)' in df.columns:
    plt.figure(figsize=(12, 6))
    sns.scatterplot(data=df, x='Density\n(P/Km2)', y='Life expectancy')
    plt.title("Esp√©rance de vie en fonction de la densit√© de population")
    plt.xlabel("Densit√© de population (P/Km¬≤)")
    plt.ylabel("Esp√©rance de vie")
    plt.tight_layout()
    plt.show()

# Scatter plot pour la proportion de terres agricoles
if 'Agricultural Land( %)' in df.columns:
    plt.figure(figsize=(12, 6))
    sns.scatterplot(data=df, x='Agricultural Land( %)', y='Life expectancy')
    plt.title("Esp√©rance de vie en fonction de la proportion de terres agricoles")
    plt.xlabel("Proportion de terres agricoles (%)")
    plt.ylabel("Esp√©rance de vie")
    plt.tight_layout()
    plt.show()

# Scatter plot pour la taille des forces arm√©es (Armed Forces size)
if 'Armed Forces size' in df.columns:
    plt.figure(figsize=(12, 6))
    sns.scatterplot(data=df, x='Armed Forces size', y='Life expectancy')
    plt.title("Esp√©rance de vie en fonction de la taille des forces arm√©es")
    plt.xlabel("Taille des forces arm√©es")
    plt.ylabel("Esp√©rance de vie")
    plt.tight_layout()
    plt.show()

# Scatter plot pour les d√©penses de sant√© √† la charge des m√©nages (Out of pocket health expenditure)
if 'Out of pocket health expenditure' in df.columns:
    plt.figure(figsize=(12, 6))
    sns.scatterplot(data=df, x='Out of pocket health expenditure', y='Life expectancy')
    plt.title("Esp√©rance de vie en fonction des d√©penses de sant√© √† la charge des m√©nages")
    plt.xlabel("D√©penses de sant√© √† la charge des m√©nages")
    plt.ylabel("Esp√©rance de vie")
    plt.tight_layout()
    plt.show()


# # Analyse multivari√©e (Pairplot + Regression Lineaire Multiple)
# # airplot avec les variables les plus influentes :
# from seaborn import pairplot

# selected_vars = [target] + list(correlations.index)
# sns.pairplot(df[selected_vars], diag_kind='kde')
# plt.suptitle("Analyse multivari√©e des variables li√©es √† Life expectancy", y=1.02)
# plt.show()


#R√©gression lin√©aire multiple (simple aper√ßu avant la mod√©lisation compl√®te) :

import statsmodels.api as sm

# Variables explicatives
X = df[correlations.index]
y = df[target]

# Ajout de l'intercept
X = sm.add_constant(X)

# Mod√®le OLS
model = sm.OLS(y, X).fit()

# R√©sum√©
print(model.summary())



# In[53]:


# 6. Clustering (Non Supervis√©)
# Standardisation des donn√©es
# Application du PCA (visualisation 2D)
# K-Means : choix du nombre optimal de clusters (m√©thode du coude)
# Hierarchical Clustering + Dendrogramme
# Interpr√©tation des clusters obtenus


# In[54]:


# ========= Normalisation (StandardScaler) =========
scaler = StandardScaler()
df_std = pd.DataFrame(scaler.fit_transform(df[num_cols]), columns=num_cols, index=df.index)

# ========= PCA (visualisation 2D) =========
pca = PCA(n_components=2)
X_pca = pca.fit_transform(df_std)

pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df.index)

plt.figure(figsize=(8,6))
plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.7)
plt.title('Projection PCA des pays (2 composantes principales)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.grid(True)
plt.show()

# ========= K-Means ‚Äì m√©thode du coude =========
inertia = []
K_range = range(1, 11)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42, n_init='auto')
    km.fit(df_std)
    inertia.append(km.inertia_)

plt.figure(figsize=(8,5))
plt.plot(K_range, inertia, 'o-')
plt.xlabel('Nombre de clusters (k)')
plt.ylabel("Inertie intra-cluster")
plt.title("M√©thode du Coude")
plt.grid(True)
plt.show()

# ========= K-Means final =========
k_optimal = 4  # adapte en fonction du coude
kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init='auto')
clusters = kmeans.fit_predict(df_std)

df['Cluster_KMeans'] = clusters
pca_df['Cluster_KMeans'] = clusters  # ajouter aussi dans pca_df pour l'affichage

# Visualisation PCA + Clustering
plt.figure(figsize=(8,6))
sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Cluster_KMeans', palette='Set2')
plt.title(f'K-Means Clustering (k={k_optimal}) - PCA')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

# ========= Clustering Hi√©rarchique + Dendrogramme =========
linkage_matrix = linkage(df_std, method='ward')

plt.figure(figsize=(12, 5))
dendrogram(linkage_matrix, truncate_mode='lastp', p=20, leaf_rotation=45, leaf_font_size=12, show_contracted=True)
plt.title("Dendrogramme ‚Äì Clustering Hi√©rarchique")
plt.xlabel("Pays ou groupes")
plt.ylabel("Distance")
plt.grid(True)
plt.show()

n_clusters = 4
labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')
df['Cluster_Hierarchical'] = labels

# ========= Interpr√©tation des clusters =========
cluster_summary = df.groupby('Cluster_KMeans').mean(numeric_only=True)
print("R√©sum√© par Cluster KMeans :\n", cluster_summary)

plt.figure(figsize=(8,5))
sns.boxplot(data=df, x='Cluster_KMeans', hue='Cluster_KMeans', y='Life expectancy', palette='Set2')
plt.title("Esp√©rance de vie par Cluster KMeans")
plt.xlabel("Cluster")
plt.ylabel("Life Expectancy")
plt.grid(True)
plt.show()


# In[55]:


# 7. Mod√©lisation Pr√©dictive (R√©gression)
# D√©finir X (features) et y (target = Life Expectancy)
# S√©paration train/test
# Mod√®les :
# R√©gression Lin√©aire
# Random Forest Regressor
# √âvaluation des mod√®les :
# RMSE, MAE, R¬≤
# Graphique : pr√©dictions vs valeurs r√©elles


# In[56]:


# S√©lectionner les variables explicatives (features) et la variable cible (target)
X = df[correlations.index]  # Les variables les plus corr√©l√©es √† 'Life expectancy'
y = df[target]  # La variable cible : Life expectancy

# S√©paration train/test (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. R√©gression Lin√©aire
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# Pr√©dictions
y_pred_lin = lin_reg.predict(X_test)

# 2. Random Forest Regressor
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)

# Pr√©dictions
y_pred_rf = rf_reg.predict(X_test)

# 3. √âvaluation des Mod√®les
# RMSE, MAE, R¬≤ pour les deux mod√®les

# √âvaluation pour R√©gression Lin√©aire
rmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))
mae_lin = mean_absolute_error(y_test, y_pred_lin)
r2_lin = r2_score(y_test, y_pred_lin)

# √âvaluation pour Random Forest
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Affichage des r√©sultats
print("üìä √âvaluation des Mod√®les:")
print("\nR√©gression Lin√©aire:")
print(f"RMSE: {rmse_lin:.2f}")
print(f"MAE: {mae_lin:.2f}")
print(f"R¬≤: {r2_lin:.2f}")

print("\nRandom Forest Regressor:")
print(f"RMSE: {rmse_rf:.2f}")
print(f"MAE: {mae_rf:.2f}")
print(f"R¬≤: {r2_rf:.2f}")

# 4. Graphique : Pr√©dictions vs Valeurs R√©elles

plt.figure(figsize=(12, 6))

# Graphique pour la r√©gression lin√©aire
plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred_lin, alpha=0.6, color='skyblue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k')  # Redundant 'color' removed
plt.title('R√©gression Lin√©aire: Pr√©dictions vs Valeurs R√©elles')
plt.xlabel('Valeurs R√©elles')
plt.ylabel('Pr√©dictions')

# Graphique pour Random Forest
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred_rf, alpha=0.6, color='lightgreen')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k')  # Redundant 'color' removed
plt.title('Random Forest: Pr√©dictions vs Valeurs R√©elles')
plt.xlabel('Valeurs R√©elles')
plt.ylabel('Pr√©dictions')

plt.tight_layout()
plt.show()


# In[57]:


# Conclusion :

# Ce projet a permis de pr√©dire l'esp√©rance de vie par pays en utilisant des donn√©es d√©mographiques,
# √©conomiques et de sant√©. Nous avons appliqu√© des techniques de pr√©traitement, telles que le nettoyage 
# des donn√©es et la normalisation, et utilis√© des mod√®les de r√©gression pour effectuer les pr√©dictions. 
# Les r√©sultats montrent que notre mod√®le offre une pr√©cision raisonnable, mais des am√©liorations peuvent 
# √™tre apport√©es en utilisant des mod√®les plus complexes. Ce travail d√©montre l'application des techniques 
# d'intelligence artificielle √† un probl√®me concret et met en √©vidence l'importance d'une bonne pr√©paration 
# des donn√©es pour obtenir des r√©sultats fiables.


# In[ ]:




